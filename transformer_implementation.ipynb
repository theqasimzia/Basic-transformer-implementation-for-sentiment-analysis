{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d492811e-57ef-43df-bfe7-1cf6a361c801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset_path = 'reviews.csv'\n",
    "df = pd.read_csv(dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4eb7b127-30ba-418d-a231-15b2fb95a693",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5856a180-7258-474c-a438-ebba5cf1ee8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tensorflow\n",
      "Version: 2.15.0\n",
      "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
      "Home-page: https://www.tensorflow.org/\n",
      "Author: Google Inc.\n",
      "Author-email: packages@tensorflow.org\n",
      "License: Apache 2.0\n",
      "Location: C:\\Users\\TECHMALANG\\anaconda3\\Lib\\site-packages\n",
      "Requires: tensorflow-intel\n",
      "Required-by: \n",
      "---\n",
      "Name: pandas\n",
      "Version: 2.1.4\n",
      "Summary: Powerful data structures for data analysis, time series, and statistics\n",
      "Home-page: https://pandas.pydata.org\n",
      "Author: \n",
      "Author-email: The Pandas Development Team <pandas-dev@python.org>\n",
      "License: BSD 3-Clause License\n",
      "\n",
      "Copyright (c) 2008-2011, AQR Capital Management, LLC, Lambda Foundry, Inc. and PyData Development Team\n",
      "All rights reserved.\n",
      "\n",
      "Copyright (c) 2011-2023, Open source contributors.\n",
      "\n",
      "Redistribution and use in source and binary forms, with or without\n",
      "modification, are permitted provided that the following conditions are met:\n",
      "\n",
      "* Redistributions of source code must retain the above copyright notice, this\n",
      "  list of conditions and the following disclaimer.\n",
      "\n",
      "* Redistributions in binary form must reproduce the above copyright notice,\n",
      "  this list of conditions and the following disclaimer in the documentation\n",
      "  and/or other materials provided with the distribution.\n",
      "\n",
      "* Neither the name of the copyright holder nor the names of its\n",
      "  contributors may be used to endorse or promote products derived from\n",
      "  this software without specific prior written permission.\n",
      "\n",
      "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
      "AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
      "IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
      "DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
      "FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
      "DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
      "SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
      "CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
      "OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
      "OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
      "Location: C:\\Users\\TECHMALANG\\anaconda3\\Lib\\site-packages\n",
      "Requires: numpy, python-dateutil, pytz, tzdata\n",
      "Required-by: altair, bokeh, datashader, holoviews, hvplot, panel, seaborn, statsmodels, streamlit, xarray\n",
      "---\n",
      "Name: scikit-learn\n",
      "Version: 1.2.2\n",
      "Summary: A set of python modules for machine learning and data mining\n",
      "Home-page: http://scikit-learn.org\n",
      "Author: \n",
      "Author-email: \n",
      "License: new BSD\n",
      "Location: C:\\Users\\TECHMALANG\\anaconda3\\Lib\\site-packages\n",
      "Requires: joblib, numpy, scipy, threadpoolctl\n",
      "Required-by: imbalanced-learn\n",
      "---\n",
      "Name: matplotlib\n",
      "Version: 3.8.0\n",
      "Summary: Python plotting package\n",
      "Home-page: https://matplotlib.org\n",
      "Author: John D. Hunter, Michael Droettboom\n",
      "Author-email: matplotlib-users@python.org\n",
      "License: PSF\n",
      "Location: C:\\Users\\TECHMALANG\\anaconda3\\Lib\\site-packages\n",
      "Requires: contourpy, cycler, fonttools, kiwisolver, numpy, packaging, pillow, pyparsing, python-dateutil\n",
      "Required-by: seaborn\n"
     ]
    }
   ],
   "source": [
    "!pip show tensorflow pandas scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83c0b025-bac0-42a5-b7ac-031000f698be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\TECHMALANG\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd56686c-6837-4f4e-a738-76b57611a687",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df['Text'])\n",
    "\n",
    "# Convert text to sequences of integers\n",
    "sequences = tokenizer.texts_to_sequences(df['Text'])\n",
    "\n",
    "# Pad the sequences to ensure uniform length\n",
    "max_length = max(len(x) for x in sequences)  # You might want to set a specific max_length\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "278f677f-4754-4073-9044-a15ba83fe2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert scores to binary labels\n",
    "df['Sentiment'] = np.where(df['Score'] > 3, 1, 0)  # 1 for positive, 0 for negative\n",
    "labels = df['Sentiment'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ee980e6-f631-4b28-bbf8-399289ea84f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40be8f20-1ab9-4ddd-b91f-afda0a7d03cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                            np.arange(d_model)[np.newaxis, :],\n",
    "                            d_model)\n",
    "    # Apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    # Apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28d6ee93-3c3a-4483-8c82-c7be2c1e11fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\TECHMALANG\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\TECHMALANG\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding, Dense, LayerNormalization, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Model hyperparameters (you may experiment with these)\n",
    "vocab_size = 10000\n",
    "max_length = 128  # Adjust based on your padding length\n",
    "d_model = 64  # The dimensionality of the embedding/output vectors\n",
    "\n",
    "model = Sequential([\n",
    "    # Embedding layer\n",
    "    Embedding(vocab_size, d_model, input_length=max_length),\n",
    "    # Positional Encoding (addition operation)\n",
    "    # Note: TensorFlow doesn't have a built-in layer for positional encoding; you might need to add it manually or customize a layer.\n",
    "    # Simplified for demonstration: we'll skip adding it directly in this code snippet.\n",
    "    \n",
    "    # Here you would include the attention and feedforward layers, which are omitted for simplicity\n",
    "    \n",
    "    # Output layer\n",
    "    Dense(1, activation='sigmoid')  # Assuming binary classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eedceb77-2312-4a29-8e4a-48526b878041",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncodingLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncodingLayer, self).__init__()\n",
    "        self.pos_encoding = positional_encoding(position, d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        return x + self.pos_encoding[:, :tf.shape(x)[1], :]\n",
    "\n",
    "position = max_length  # This should match your input sequence length\n",
    "d_model = 64  # Dimensionality of the embedding/output vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfcf130a-f7fc-4daa-bd83-50ea579fa58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 128)]                0         []                            \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, 128, 64)              640000    ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " positional_encoding_layer   (None, 128, 64)              0         ['embedding_1[0][0]']         \n",
      " (PositionalEncodingLayer)                                                                        \n",
      "                                                                                                  \n",
      " attention (Attention)       (None, 128, 64)              0         ['positional_encoding_layer[0]\n",
      "                                                                    [0]',                         \n",
      "                                                                     'positional_encoding_layer[0]\n",
      "                                                                    [0]']                         \n",
      "                                                                                                  \n",
      " global_average_pooling1d (  (None, 64)                   0         ['attention[0][0]']           \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 64)                   0         ['global_average_pooling1d[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 1)                    65        ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 640065 (2.44 MB)\n",
      "Trainable params: 640065 (2.44 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, Dense, Attention, GlobalAveragePooling1D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Inputs\n",
    "inputs = Input(shape=(max_length,))\n",
    "\n",
    "# Embeddings\n",
    "embedding_layer = Embedding(vocab_size, d_model)(inputs)\n",
    "\n",
    "# Positional Encoding\n",
    "pos_encoding_layer = PositionalEncodingLayer(position=max_length, d_model=d_model)(embedding_layer)\n",
    "\n",
    "# Self-Attention (simplified)\n",
    "attention_output = Attention()([pos_encoding_layer, pos_encoding_layer])\n",
    "\n",
    "# Pooling + Output\n",
    "x = GlobalAveragePooling1D()(attention_output)\n",
    "x = Dropout(0.1)(x)\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Model\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af3a70cc-bb7a-4022-98aa-13cf65ea828a",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 5.94 GiB for an array with shape (454763, 3507) and data type int32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(X_test, y_test))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:91\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Converts the given `value` to an `EagerTensor`.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03mNote that this function could return cached copies of created constants for\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;124;03m  TypeError: if `dtype` is not compatible with the type of t.\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m     88\u001b[0m   \u001b[38;5;66;03m# Make a copy explicitly because the EagerTensor might share the underlying\u001b[39;00m\n\u001b[0;32m     89\u001b[0m   \u001b[38;5;66;03m# memory with the input array. Without this copy, users will be able to\u001b[39;00m\n\u001b[0;32m     90\u001b[0m   \u001b[38;5;66;03m# modify the EagerTensor after its creation by changing the input array.\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m   value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, ops\u001b[38;5;241m.\u001b[39mEagerTensor):\n\u001b[0;32m     93\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m dtype:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 5.94 GiB for an array with shape (454763, 3507) and data type int32"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00269ee2-e1f6-4863-a909-2ceb28bd16dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Define a sensible maximum sequence length\n",
    "max_length = 128\n",
    "\n",
    "# Pad the sequences\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3a8be1-ed83-4414-b1e1-95d165b0cc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming 'labels' contains your binary sentiment labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ec3936-33dc-4013-ba62-f12cb7553fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, Dense, GlobalAveragePooling1D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Inputs\n",
    "inputs = Input(shape=(max_length,))\n",
    "\n",
    "# Embeddings\n",
    "embedding_layer = Embedding(vocab_size, d_model, input_length=max_length)(inputs)\n",
    "\n",
    "# Add any additional layers you might have defined earlier\n",
    "# For example, Positional Encoding and Attention layers (omitted here for brevity)\n",
    "\n",
    "# Output layer\n",
    "x = GlobalAveragePooling1D()(embedding_layer)  # Adjusted for simplicity; replace with your actual layers\n",
    "x = Dropout(0.1)(x)\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Model compilation\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1e5c9b-3f23-40f8-9001-54ec8face16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff6418c-37a5-4965-9259-140fa96de7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "\n",
    "# Assuming you have defined `d_model` and `num_heads`\n",
    "num_heads = 8  # Number of attention heads\n",
    "\n",
    "# In your model architecture\n",
    "multi_head_attention = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "attention_output = multi_head_attention(query=embedding_layer, value=embedding_layer, key=embedding_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f6e1d7-a204-4632-91d2-ad810f4947eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer, Dense, LayerNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "class TransformerEncoderLayer(Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        self.dense_proj = Sequential([Dense(d_model, activation='relu'), Dense(d_model)])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        attn_output = self.multi_head_attention(inputs, inputs, inputs)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.dense_proj(out1)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "# Example usage\n",
    "num_encoder_layers = 4  # Number of encoder layers\n",
    "\n",
    "# Initialize a list to hold the encoder layers\n",
    "encoder_layers = [TransformerEncoderLayer(d_model=d_model, num_heads=num_heads) for _ in range(num_encoder_layers)]\n",
    "\n",
    "# In your model definition, add each encoder layer\n",
    "x = embedding_layer  # Assuming `embedding_layer` is defined\n",
    "for encoder_layer in encoder_layers:\n",
    "    x = encoder_layer(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d4e218-530d-41c6-bdfe-9410f32a4192",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_reviews = [\"This product was great, I loved it!\", \"Not what I expected, quite disappointing.\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53844856-65bc-4057-bf96-79caaf9a7f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the new reviews\n",
    "new_sequences = tokenizer.texts_to_sequences(new_reviews)\n",
    "\n",
    "# Pad the new sequences\n",
    "new_padded_sequences = pad_sequences(new_sequences, maxlen=max_length, padding='post', truncating='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ee82a2-b2d1-4c7e-a0cc-0b519b569ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Assuming 'df' is your DataFrame and 'Text' is the column containing text data\n",
    "texts = df['Text'].astype(str).tolist()  # Convert the text column to a list of strings\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(texts)  # Fit the tokenizer on the texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e36ce99-92d3-42c4-a2de-ceb686a11ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(new_padded_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e754a3a-312a-441a-b029-f01155ccaf59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
